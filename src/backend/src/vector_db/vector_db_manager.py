# vector_db_manager.py
# -*- coding: utf-8 -*-

from __future__ import annotations

import os
import math
from typing import Any, Dict, List, Optional, Tuple
from tqdm import tqdm
import warnings
import hashlib
warnings.filterwarnings("ignore", message="BertForMaskedLM has generative capabilities")

from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance,
    VectorParams,
    SparseVectorParams,
    SparseIndexParams,
    PointStruct,
    Filter,
    FieldCondition,
    MatchValue,
    NamedVector,
    FilterSelector,
    SparseVector,
)

from qdrant_client.http.models import ScoredPoint

# ---- ì™¸ë¶€ ëª¨ë“ˆ(ì´ë¯¸ ì£¼ì–´ì§„ ì½”ë“œë“¤) ----
from config import QDRANT_URL, QDRANT_API_KEY, DISTANCE, FORMATS
from embedding import model as dense_sbert_model
from embedding import content_embedder
from sparse_helper import bm25_encode, bm25_fit
from splade_module import splade_encode
from vector_db_helper import (
    create_doc_upsert,
    query_unique_docs as qdrant_query_unique_docs,
)
from reranker_module import (
    weighted_fuse,
    deduplicate_and_average as dedup_avg_by_doc,
    load_cross_encoder,
    rerank_with_cross_encoder,
    apply_date_window_boost,  # ë‚ ì§œ ë¶€ìŠ¤íŒ…(ë‹¨ì¼ ì»¬ë ‰ì…˜ ë²„ì „)
)

# -------------------------------------------------------
# VectorDBManager
# -------------------------------------------------------
class VectorDBManager:
    """
    Qdrant ê¸°ë°˜ ë©€í‹° ì»¬ë ‰ì…˜ ë²¡í„° DB ë§¤ë‹ˆì €
    - Create: ì»¬ë ‰ì…˜ ìƒì„±/ì´ˆê¸°í™”
    - Read: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Dense + BM25 + SPLADE) â†’ ê°€ì¤‘ ê²°í•© â†’ (ì˜µì…˜)Cross-Encoder â†’ (ì˜µì…˜)Date-Boost
    - Update: ë¬¸ì„œ ì—…ë°ì´íŠ¸(ì¬ì„ë² ë”© ì˜µì…˜)
    - Delete: ë¬¸ì„œ ì‚­ì œ
    """

    # ------------ ì´ˆê¸°í™” ------------
    def __init__(
        self,
        url: Optional[str] = None,
        api_key: Optional[str] = None,
        default_collection: Optional[str] = None,
        pipeline_config: Optional[Dict[str, Any]] = None,
    ):
        self.client = QdrantClient(
            url=url or QDRANT_URL, api_key=api_key or QDRANT_API_KEY
        )
        self.default_collection = default_collection or "notion.marketing"

        # ì„ë² ë”© ëª¨ë¸ í•¸ë“¤
        self.embedding_models: Dict[str, Any] = {
            "dense": dense_sbert_model,  # SentenceTransformer
            # sparse(BM25)ì™€ spladeëŠ” í•¨ìˆ˜í˜• ì¸ì½”ë”ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        }

        # Cross-Encoder (í•„ìš” ì‹œ ì§€ì—° ì´ˆê¸°í™”)
        self._cross_tokenizer = None
        self._cross_model = None
        self._cross_model_name = None
        from sparse_helper import bm25_encode
        from splade_module import splade_encode

        self.dense_model = dense_sbert_model
        self.bm25_encode = bm25_encode
        self.splade_encode = splade_encode

        # íŒŒì´í”„ë¼ì¸ ì„¤ì • ê¸°ë³¸ê°’
        self.pipeline_config: Dict[str, Any] = {
            "use_dense": True,
            "use_sparse": True,
            "use_splade": True,
            "use_reranker": True,
            "use_date_boost": True,
            "dense_weight": 0.6,
            "sparse_weight": 0.3,
            "splade_weight": 0.1,
            "cross_encoder_model": "Dongjin-kr/ko-reranker",
            # Date boost ê¸°ë³¸ íŒŒë¼ë¯¸í„°
            "date_decay_rate": 0.03,
            "date_weight": 0.45,
            # ë‚ ì§œ ìœˆë„ìš°(ì˜µì…˜): ë¬¸ìì—´ ISO8601 ë˜ëŠ” None
            "date_from": None,
            "date_to": None,
        }
        if pipeline_config:
            self.pipeline_config.update(pipeline_config)

    # ------------ Create / Init ------------
    def create_collection(
        self,
        name: str,
        vector_size: int,
        distance: str | Distance = "Cosine",
        force: bool = False,
        include_sparse: bool = True,
        include_splade: bool = True,
    ) -> Dict[str, Any]:
        """
        Qdrantì— ë©€í‹°-ë²¡í„° ìŠ¤í‚¤ë§ˆë¡œ ì»¬ë ‰ì…˜ ìƒì„± (dense + sparse + splade)
        """
        if isinstance(distance, str):
            distance = {
                "Cosine": Distance.COSINE,
                "Dot": Distance.DOT,
                "Euclid": Distance.EUCLID,
            }.get(distance, Distance.COSINE)

        if force:
            try:
                self.client.delete_collection(name)
            except Exception:
                pass

        vectors_cfg = {"dense": VectorParams(size=vector_size, distance=distance)}
        sparse_cfg = {}
        if include_sparse:
            sparse_cfg["sparse"] = SparseVectorParams(
                index=SparseIndexParams(on_disk=False)
            )
        if include_splade:
            sparse_cfg["splade"] = SparseVectorParams(
                index=SparseIndexParams(on_disk=False)
            )

        self.client.recreate_collection(
            collection_name=name,
            vectors_config=vectors_cfg,
            sparse_vectors_config=sparse_cfg if sparse_cfg else None,
        )
        return {"name": name, "vector_size": vector_size, "distance": str(distance)}

    def initialize_collections(self, config: Dict[str, Dict[str, Any]]) -> None:
        """
        ì—¬ëŸ¬ ì»¬ë ‰ì…˜ì„ í•œ ë²ˆì— ì´ˆê¸°í™”
        Example:
        {
            "notion.marketing": {"vector_size": 768, "distance": "Cosine"},
            "notion.notice": {"vector_size": 768, "distance": "Cosine"}
        }
        """
        for col_name, spec in config.items():
            self.create_collection(
                name=col_name,
                vector_size=spec.get("vector_size", 768),
                distance=spec.get("distance", "Cosine"),
                force=spec.get("force", False),
                include_sparse=True,
                include_splade=True,
            )

    # ------------ BM25 í•™ìŠµ í—¬í¼ ------------
    def fit_bm25_from_json_folder(self, base_path: str) -> int:
        """
        í´ë” ë‚´ ëª¨ë“  JSONì—ì„œ content/contentsë¥¼ ëª¨ì•„ BM25 ë²¡í„°ë¼ì´ì € í•™ìŠµ
        """
        import json

        all_texts: List[str] = []
        for root, _, files in os.walk(base_path):
            for file in files:
                if not file.endswith(".json"):
                    continue
                try:
                    with open(os.path.join(root, file), "r", encoding="utf-8") as f:
                        data = json.load(f)
                        text = data.get("content") or data.get("contents")
                        if text:
                            all_texts.append(text)
                except Exception:
                    continue

        if not all_texts:
            raise RuntimeError("[BM25] No text data found for fitting")
        bm25_fit(all_texts)
        return len(all_texts)

    # ------------ Upsert / Update / Delete ------------
    def make_doc_hash_id(self, file_path: str):
        """
        íŒŒì¼ì˜ ìƒëŒ€ê²½ë¡œë¥¼ í•´ì‹œí•´ì„œ Qdrant ê³ ìœ  ID ìƒì„±
        (ì ˆëŒ€ê²½ë¡œê°€ ë‹¬ë¼ë„ ë™ì¼ í´ë” êµ¬ì¡°ë©´ ê°™ì€ ê²°ê³¼ë¥¼ ë³´ì¥)
        """
        rel_path = os.path.relpath(file_path, start=os.getcwd())
        return hashlib.md5(rel_path.encode("utf-8")).hexdigest()

    def upsert_folder(self, folder_path: str, collection_name: str):
        """
        í´ë” ë‚´ ëª¨ë“  JSON ë¬¸ì„œë¥¼ ì½ì–´ ìë™ìœ¼ë¡œ ë²¡í„° ìƒì„± í›„ ì—…ì„œíŠ¸
        """
        import os, json
        from tqdm import tqdm

        if not os.path.exists(folder_path):
            print(f"Folder not found: {folder_path}")
            return

        files = [f for f in os.listdir(folder_path) if f.endswith(".json")]
        if not files:
            print(f"No JSON files in {folder_path}")
            return

        print(f"ğŸ“‚ Upserting data from: {folder_path}")
        print(f"[INFO] Upserting {len(files)} documents into {collection_name}")

        for file_name in tqdm(files, desc=f"Upserting â†’ {collection_name}"):
            file_path = os.path.join(folder_path, file_name)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

                doc_id = self.make_doc_hash_id(file_path)
                self.upsert_document(collection_name, data, doc_id)
            except Exception as e:
                print(f"[ERROR] Failed to upsert {file_name}: {e}")

    def upsert_document(self, collection_name, data, doc_id):
        """
        ë‹¨ì¼ ë¬¸ì„œë¥¼ Qdrantì— ì—…ì„œíŠ¸í•©ë‹ˆë‹¤.
        - content(ë˜ëŠ” contents) í•„ë“œë¡œë¶€í„° Dense, BM25, SPLADE ë²¡í„°ë¥¼ ìë™ ìƒì„±
        - ëª¨ë“  ë²¡í„°ëŠ” NamedVectorë¡œ ì €ì¥ë¨
        """
        from qdrant_client.models import PointStruct, SparseVector

        content = data.get("content") or data.get("contents") or ""
        title = data.get("title", "")

        # Dense embedding (SentenceTransformer)
        dense_vec = self.dense_model.encode(content)

        # Sparse embeddings
        bm25_vec = self.bm25_encode(content)
        splade_vec = self.splade_encode(content)

        # BM25, SPLADEì´ dictì´ë©´ SparseVectorë¡œ ë³€í™˜
        if isinstance(bm25_vec, dict):
            bm25_vec = SparseVector(
                indices=list(bm25_vec.keys()),
                values=list(bm25_vec.values())
            )
        if isinstance(splade_vec, dict):
            splade_vec = SparseVector(
                indices=list(splade_vec.keys()),
                values=list(splade_vec.values())
            )

        # ë¬¸ì„œ ì›ë³¸ ê²½ë¡œ / id ì¶”ê°€
        payload = {
            **data,
            "title": title,
            "id": doc_id,
            "parent_id": doc_id
        }

        # Qdrant ì—…ì„œíŠ¸
        self.client.upsert(
            collection_name=collection_name,
            points=[
                PointStruct(
                    id=doc_id,
                    vector={
                        "dense": dense_vec,
                        "sparse": bm25_vec,
                        "splade": splade_vec
                    },
                    payload=payload
                )
            ]
        )

    def delete_document(self, collection: str, doc_id: Any) -> bool:
        """
        ë¬¸ì„œ ì‚­ì œ:
        - parent_id==doc_id ì¡°ê±´ìœ¼ë¡œ ëª¨ë“  ì²­í¬ ì‚­ì œ
        """
        try:
            self.client.delete(
                collection_name=collection,
                points_selector=FilterSelector(
                    filter=Filter(must=[FieldCondition(key="parent_id", match=MatchValue(value=doc_id))])
                ),
            )
            return True
        except Exception:
            return False

    # ------------ ì¡°íšŒ ìœ í‹¸ ------------
    def get_document_by_id(self, collection: str, doc_id: Any) -> Optional[Dict[str, Any]]:
        """
        parent_id == doc_idì¸ ì²« í¬ì¸íŠ¸ì˜ payloadë¥¼ ëŒ€í‘œë¡œ ë°˜í™˜
        """
        hits, _ = self.client.scroll(
            collection_name=collection,
            scroll_filter=Filter(must=[FieldCondition(key="parent_id", match=MatchValue(value=doc_id))]),
            limit=1,
            with_payload=True,
        )
        if not hits:
            return None
        p = hits[0]
        return {"id": doc_id, "payload": p.payload}

    def get_top_documents(self, results: List[Dict[str, Any]], top_n: int = 10) -> List[Dict[str, Any]]:
        return results[:top_n]

    # ------------ ë‚´ë¶€: ì»¬ë ‰ì…˜ ë‹¨ìœ„ ê²€ìƒ‰ ------------
    def _search_collection_unique(
        self,
        collection: str,
        query_text: str,
        top_k: int,
        use_dense: bool,
        use_sparse: bool,
        use_splade: bool,
    ) -> Tuple[List[ScoredPoint], List[ScoredPoint], List[ScoredPoint]]:
        """
        í•œ ì»¬ë ‰ì…˜ì— ëŒ€í•´ Dense/BM25/SPLADEë¡œ ê³ ìœ  ë¬¸ì„œ top_kë¥¼ ê°ê° ê°€ì ¸ì˜´
        """
        dense_results: List[ScoredPoint] = []
        sparse_results: List[ScoredPoint] = []
        splade_results: List[ScoredPoint] = []

        # Dense
        if use_dense:
            dense_vec = self.embedding_models["dense"].encode(query_text)
            dense_results = qdrant_query_unique_docs(
                client=self.client,
                collection_name=collection,
                query=dense_vec,
                using="dense",
                top_k=top_k,
            )

        # BM25
        if use_sparse:
            bm25_vec = bm25_encode(query_text)
            if isinstance(bm25_vec, dict):
                bm25_vec = SparseVector(
                    indices=list(bm25_vec.keys()), values=list(bm25_vec.values())
                )
            sparse_results = qdrant_query_unique_docs(
                client=self.client,
                collection_name=collection,
                query=bm25_vec,
                using="sparse",
                top_k=top_k,
            )

        # SPLADE
        if use_splade:
            sp_vec = splade_encode(query_text)
            if isinstance(sp_vec, dict):
                # splade_moduleì€ keyê°€ strì¼ ìˆ˜ ìˆìœ¼ë‹ˆ int ë³€í™˜
                idxs = [int(k) for k in sp_vec.keys()]
                vals = [float(v) for v in sp_vec.values()]
                sp_vec = SparseVector(indices=idxs, values=vals)
            splade_results = qdrant_query_unique_docs(
                client=self.client,
                collection_name=collection,
                query=sp_vec,
                using="splade",
                top_k=top_k,
            )

        return dense_results, sparse_results, splade_results

    # ------------ í•µì‹¬: Query íŒŒì´í”„ë¼ì¸ ------------
    def query(
        self,
        query_text: str,
        top_k: int = 10,
        collections: Optional[List[str]] = None,
        threshold: Optional[float] = None,
        use_reranker: Optional[bool] = None,
        date_from: Optional[str] = None,
        date_to: Optional[str] = None,
        date_decay_rate: Optional[float] = None,
        date_weight: Optional[float] = None,
    ) -> List[Dict[str, Any]]:
        """
        Query â†’ Dense/Sparse/SPLADE â†’ ê°€ì¤‘ ê²°í•© â†’ ë¬¸ì„œ ë‹¨ìœ„ í‰ê· /Dedup
              â†’ (ì˜µì…˜) Cross-Encoder â†’ (ì˜µì…˜) Date-Boost â†’ ê²°ê³¼
        """
        cfg = self.pipeline_config
        use_dense = cfg["use_dense"]
        use_sparse = cfg["use_sparse"]
        use_splade = cfg["use_splade"]
        use_reranker = cfg["use_reranker"] if use_reranker is None else use_reranker
        use_date_boost = cfg["use_date_boost"]

        dw = (cfg["dense_weight"], cfg["sparse_weight"], cfg["splade_weight"])

        collections = collections or [self.default_collection]
        merged_doclevel: List[Dict[str, Any]] = []

        for col in collections:
            dense_res, sparse_res, splade_res = self._search_collection_unique(
                collection=col,
                query_text=query_text,
                top_k=top_k,
                use_dense=use_dense,
                use_sparse=use_sparse,
                use_splade=use_splade,
            )

            # ëª¨ë¸ë³„ ê²°ê³¼ ê²°í•©(ì ìˆ˜ ì •ê·œí™” + ê°€ì¤‘í•©)
            fused = weighted_fuse(
                dense_res, sparse_res, splade_res, dw[0], dw[1], dw[2]
            )

            # ë¬¸ì„œ ë‹¨ìœ„ í‰ê· ì ìˆ˜ + Dedup
            doclevel = dedup_avg_by_doc(fused, client=self.client, col_name=col, top_k=top_k)
            # ì»¬ë ‰ì…˜ëª… ë¶€ì°©
            for d in doclevel:
                d["collection"] = col

            merged_doclevel.extend(doclevel)

        # ì—¬ëŸ¬ ì»¬ë ‰ì…˜ì„ í•©ì³¤ë‹¤ë©´ ë‹¨ìˆœ ìƒìœ„ top_kë¡œ ìë¥´ê¸°
        merged_doclevel.sort(key=lambda x: x["avg_score"], reverse=True)
        merged_doclevel = merged_doclevel[:top_k]

        # ---- (ì˜µì…˜) Cross-Encoder ----
        if use_reranker:
            # Cross Encoder ëª¨ë¸ ë¡œë“œ(ì§€ì—° ë¡œë”©)
            if self._cross_model is None or self._cross_model_name != cfg["cross_encoder_model"]:
                tok, mod = load_cross_encoder(cfg["cross_encoder_model"])
                self._cross_tokenizer, self._cross_model = tok, mod
                self._cross_model_name = cfg["cross_encoder_model"]

            # ê° ë¬¸ì„œì—ì„œ ëŒ€í‘œ ì²­í¬ 1ê°œë¥¼ ë½‘ì•„ candidate êµ¬ì„±
            candidates: List[Dict[str, Any]] = []
            id2meta: Dict[Any, Dict[str, Any]] = {}
            for d in merged_doclevel:
                doc_id = d["doc_id"]
                col = d["collection"]
                hits, _ = self.client.scroll(
                    collection_name=col,
                    scroll_filter=Filter(
                        must=[FieldCondition(key="parent_id", match=MatchValue(value=doc_id))]
                    ),
                    limit=1,
                )
                if not hits:
                    continue
                p = hits[0]
                text = p.payload.get("contents") or p.payload.get("content") or p.payload.get("text", "")
                title = p.payload.get("title") or d.get("title", "(no title)")
                candidates.append({"id": doc_id, "text": text, "title": title})
                id2meta[doc_id] = {
                    "title": title,
                    "collection": col,
                    "avg_score": d.get("avg_score", 0.0),
                }
            
            if not candidates:
                print("[WARN] No candidates found for reranking â€” skipping Cross-Encoder.")
                return merged_doclevel[:top_k]

            reranked = rerank_with_cross_encoder(
                query=query_text,
                docs=candidates,
                tokenizer=self._cross_tokenizer,
                model=self._cross_model,
                top_k=top_k,
                device="cpu",
            )
            # Cross ê²°ê³¼ì— ì»¬ë ‰ì…˜, í‰ê· ì ìˆ˜ ë“± ë©”íƒ€ í•©ì¹˜ê¸°
            final_after_ce: List[Dict[str, Any]] = []
            for r in reranked:
                meta = id2meta.get(r["id"], {})
                final_after_ce.append(
                    {
                        "id": r["id"],
                        "title": r.get("title"),
                        "score": r.get("score", 0.0),
                        "collection": meta.get("collection", self.default_collection),
                        "avg_score": meta.get("avg_score", 0.0),
                    }
                )
        else:
            # Cross-Encoder ë¯¸ì‚¬ìš© ì‹œ, avg_score ê¸°ë°˜ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            final_after_ce = [
                {
                    "id": d["doc_id"],
                    "title": d.get("title"),
                    "score": float(d.get("avg_score", 0.0)),  # 'score' ìŠ¬ë¡¯ì— avg_score íˆ¬ì…
                    "collection": d["collection"],
                    "avg_score": float(d.get("avg_score", 0.0)),
                }
                for d in merged_doclevel
            ]

        # ---- (ì˜µì…˜) Date-Boost ----
        if use_date_boost:
            # ìš”ì²­ ì¸ì ìš°ì„ , ì—†ìœ¼ë©´ cfg ì‚¬ìš©
            df = date_from if date_from is not None else cfg.get("date_from")
            dt = date_to if date_to is not None else cfg.get("date_to")
            decay = date_decay_rate if date_decay_rate is not None else cfg.get("date_decay_rate", 0.03)
            w = date_weight if date_weight is not None else cfg.get("date_weight", 0.45)

            # ì—¬ëŸ¬ ì»¬ë ‰ì…˜ ì„ì—¬ìˆìœ¼ë©´ ì»¬ë ‰ì…˜ë³„ë¡œ ë‚ ì§œë¶€ìŠ¤íŒ… ì ìš© í›„ í•©ì¹˜ê¸°
            by_col: Dict[str, List[Dict[str, Any]]] = {}
            for r in final_after_ce:
                by_col.setdefault(r["collection"], []).append(r)

            boosted_merged: List[Dict[str, Any]] = []
            for col, subset in by_col.items():
                boosted = apply_date_window_boost(
                    results=subset,
                    client=self.client,
                    collection_name=col,
                    date_from=df,
                    date_to=dt,
                    decay_rate=decay,
                    weight=w,
                )
                boosted_merged.extend(boosted)

            boosted_merged.sort(key=lambda x: x["final_score"], reverse=True)
            return boosted_merged[:top_k]

        # ë‚ ì§œ ë¶€ìŠ¤íŒ… ë¹„í™œì„±: Cross ê²°ê³¼ ê·¸ëŒ€ë¡œ
        # ì •ê·œí™” ì—†ì´ score ë‚´ë¦¼ì°¨ìˆœ
        final_after_ce.sort(key=lambda x: x["score"], reverse=True)
        return final_after_ce[:top_k]

    # ------------ í‚¤ì›Œë“œ ê²€ìƒ‰(ê°„ë‹¨ BM25 ëŒ€ìš©) ------------
    def keyword_search(
        self,
        keyword: str,
        collections: Optional[List[str]] = None,
        limit: int = 20,
    ) -> List[Dict[str, Any]]:
        """
        Qdrantì˜ payload í•„ë“œ ì§ì ‘ 'ë¶€ë¶„ ë¬¸ìì—´ ê²€ìƒ‰'ì€ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
        BM25(Sparse) ì¿¼ë¦¬ë¡œ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ê·¼ì‚¬í•œë‹¤.
        """
        collections = collections or [self.default_collection]
        results: List[Dict[str, Any]] = []

        bm25_vec = bm25_encode(keyword)
        spv = SparseVector(indices=list(bm25_vec.keys()), values=list(bm25_vec.values()))

        for col in collections:
            hits = self.client.query_points(
                collection_name=col, query=spv, using="sparse", limit=limit
            )
            points = hits.points if hasattr(hits, "points") else hits
            for p in points:
                pid = getattr(p, "id", None)
                title = p.payload.get("title")
                results.append(
                    {
                        "id": pid,
                        "title": title,
                        "score": float(p.score),
                        "collection": col,
                    }
                )

        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:limit]

    # ------------ Reranker ë‹¨ë… ì‹¤í–‰ ------------
    def rerank_results(
        self,
        query_text: str,
        docs: List[Dict[str, Any]],
        method: str = "cross_encoder",
        top_k: int = 10,
    ) -> List[Dict[str, Any]]:
        """
        ì „ë‹¬ëœ docs(list of {id,title,text})ë¥¼ ì£¼ì–´ì§„ ë°©ë²•ìœ¼ë¡œ ì¬ì •ë ¬
        """
        if method == "average":
            # ë‹¨ìˆœ í‰ê· ì ìˆ˜ë¡œ ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆë‹¤ê³  ê°€ì • â†’ ì •ë ¬ë§Œ
            docs.sort(key=lambda x: x.get("avg_score", 0.0), reverse=True)
            return docs[:top_k]

        if method == "cross_encoder":
            if self._cross_model is None or self._cross_model_name != self.pipeline_config["cross_encoder_model"]:
                tok, mod = load_cross_encoder(self.pipeline_config["cross_encoder_model"])
                self._cross_tokenizer, self._cross_model = tok, mod
                self._cross_model_name = self.pipeline_config["cross_encoder_model"]

            reranked = rerank_with_cross_encoder(
                query=query_text,
                docs=docs,
                tokenizer=self._cross_tokenizer,
                model=self._cross_model,
                top_k=top_k,
                device="cpu",
            )
            return reranked

        if method == "ensemble":
            # ì¶”í›„ MonoT5 ë“± ì¶”ê°€ì‹œ í™•ì¥ í¬ì¸íŠ¸
            raise NotImplementedError("Ensemble reranker is not implemented yet.")

        raise ValueError(f"Unknown reranker method: {method}")

    # ------------ ë‚ ì§œ ë¶€ìŠ¤íŒ…(ë˜í¼) ------------
    def apply_date_boost(
        self,
        results: List[Dict[str, Any]],
        date_from: Optional[str] = None,
        date_to: Optional[str] = None,
        decay_rate: Optional[float] = None,
        weight: Optional[float] = None,
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì»¬ë ‰ì…˜ì´ ì„ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, collectionë³„ ê·¸ë£¹í•‘ í›„ ëª¨ë“ˆ í•¨ìˆ˜ë¥¼ í˜¸ì¶œ
        """
        df = date_from or self.pipeline_config.get("date_from")
        dt = date_to or self.pipeline_config.get("date_to")
        decay = decay_rate if decay_rate is not None else self.pipeline_config.get("date_decay_rate", 0.03)
        w = weight if weight is not None else self.pipeline_config.get("date_weight", 0.45)

        by_col: Dict[str, List[Dict[str, Any]]] = {}
        for r in results:
            col = r.get("collection", self.default_collection)
            by_col.setdefault(col, []).append(r)

        boosted_merged: List[Dict[str, Any]] = []
        for col, subset in by_col.items():
            boosted = apply_date_window_boost(
                results=subset,
                client=self.client,
                collection_name=col,
                date_from=df,
                date_to=dt,
                decay_rate=decay,
                weight=w,
            )
            boosted_merged.extend(boosted)

        boosted_merged.sort(key=lambda x: x["final_score"], reverse=True)
        return boosted_merged

    # ------------ ë¡œê¹… ------------
    def log_results(
        self, results: List[Dict[str, Any]], title: str = "Results", top_n: int = 10
    ) -> None:
        print(f"\n=== {title} ===")
        for i, r in enumerate(results[:top_n], 1):
            rid = r.get("id") or r.get("doc_id")
            scr = r.get("final_score") or r.get("score") or r.get("avg_score")
            t = r.get("title")
            dt = r.get("date")
            fresh = r.get("freshness")
            col = r.get("collection", self.default_collection)
            score_fmt = f"{scr:.4f}" if isinstance(scr, (int, float)) else "0.0000"
            print(f"{i:02d}. ID={rid}, Score={score_fmt}, "
                  f"Fresh={fresh:.3f}, Date={dt}, Title={t}, Col={col}")
        print("===================================\n")


# -------------------------------------------------------
# ë¹ ë¥¸ ìˆ˜ë™ í…ŒìŠ¤íŠ¸ (ì›í•˜ë©´ ì£¼ì„ í•´ì œ)
# -------------------------------------------------------
if __name__ == "__main__":
    # 1) ë§¤ë‹ˆì € ì´ˆê¸°í™”
    mgr = VectorDBManager(default_collection="notion.marketing")

    # 2) (ì„ íƒ) BM25 í•™ìŠµ â€” JSON ë£¨íŠ¸ ê²½ë¡œ ì§€ì •
    #    mgr.fit_bm25_from_json_folder("../../../../data")

    # 3) (ì„ íƒ) ì»¬ë ‰ì…˜ ìƒì„±
    # mgr.create_collection("notion.marketing", vector_size=768, distance="Cosine", force=False)

    # 4) ì§ˆì˜
    query = "ì¸í„´ ëª¨ì§‘ ì¼ì •"
    results = mgr.query(
        query_text=query,
        top_k=10,
        collections=["notion.marketing"],  # ì—¬ëŸ¬ ê°œ ê°€ëŠ¥
        use_reranker=True,
        # ë‚ ì§œ ìœˆë„ìš°ë¥¼ ê°•ì œë¡œ ì§€ì •í•˜ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ ê°’ ì‚¬ìš©(ì—†ìœ¼ë©´ pipeline_configë¥¼ ë”°ë¦„)
        date_from="2025-10-01T00:00:00Z",
        date_to="2025-10-03T23:59:59Z",
        date_decay_rate=0.03,
        date_weight=0.45,
    )
    mgr.log_results(results, title=f"FINAL for '{query}'", top_n=10)