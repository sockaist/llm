import os
import sys
import unittest
import json
from pathlib import Path
from dotenv import load_dotenv

# í•„ìš”í•œ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë“ˆ ì„í¬íŠ¸
from qdrant_client import QdrantClient
from src.vector_db_helper import search_doc, create_doc_upsert
from src.embedding import content_embedder, model
from src.InputChecker import InputChecker, InputNormalizer, QueryMaker, FilterGenerator

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class TestRAGImplementation(unittest.TestCase):
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì • ë° í•„ìš”í•œ ê°ì²´ ì´ˆê¸°í™”"""
        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.client = QdrantClient(
            url="https://7eb854c4-8645-4c1f-ae73-609313fb8842.us-east4-0.gcp.cloud.qdrant.io",
            api_key=os.environ.get('QDRANT_API_KEY')
        )
        
        # Google API í‚¤ ì„¤ì •
        self.api_key = os.environ.get('GOOGLE_API_KEY')
        
        # ì±—ë´‡ ëª¨ë“ˆë“¤ ì´ˆê¸°í™”
        self.input_checker = InputChecker(self.api_key)
        self.input_normalizer = InputNormalizer(self.api_key)
        self.query_maker = QueryMaker(self.api_key)
        self.filter_generator = FilterGenerator(self.api_key)
        
        # í…ŒìŠ¤íŠ¸ìš© ì»¬ë ‰ì…˜ ì´ë¦„
        self.test_collection = "portal.job"
        
        # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë¬¸ì„œ
        self.sample_doc = {
            "title": "KAIST ì „ì‚°í•™ë¶€ RAG ì‹œìŠ¤í…œ ì—°êµ¬",
            "author": "ê¹€ì—°êµ¬",
            "date": "2024-01-15",
            "link": "https://example.com/rag-research",
            "content": "RAG(Retrieval-Augmented Generation)ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."
        }

    def test_document_embedding_and_storage(self):
        """ë¬¸ì„œ ì„ë² ë”© ë° ì €ì¥ í…ŒìŠ¤íŠ¸"""
        # ë¬¸ì„œ ì„ë² ë”© ë° ì €ì¥
        create_doc_upsert(self.client, self.test_collection, self.sample_doc)
        
        # ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
        count = self.client.count(collection_name=self.test_collection, exact=True).count
        self.assertGreater(count, 0)
        print(f"ì €ì¥ëœ ë¬¸ì„œ ì²­í¬ ìˆ˜: {count}")

    def test_vector_search(self):
        """ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        # ë¬¸ì„œ ì €ì¥
        create_doc_upsert(self.client, self.test_collection, self.sample_doc)
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        query = "RAG ì‹œìŠ¤í…œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
        search_results = search_doc(self.client, query, self.test_collection, k=3)
        
        self.assertGreater(len(search_results), 0)
        
        print(f"ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
        for i, hit in enumerate(search_results):
            print(f"{i+1}. ì ìˆ˜: {hit.score:.4f}")
            print(f"   ë‚´ìš©: {hit.payload.get('text', '')[:100]}...")

    def test_input_processing_pipeline(self):
        """ì…ë ¥ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
        user_input = "ìµœê·¼ KAIST ì „ì‚°í•™ë¶€ ì±„ìš© ê³µê³ ê°€ ìˆë‚˜ìš”?"
        
        # 1. ì…ë ¥ ê²€ì¦
        check_result = self.input_checker.process_query(user_input)
        print(f"ì…ë ¥ ê²€ì¦ ê²°ê³¼: {check_result}")
        
        if check_result.get('is_valid', False):
            # 2. ì…ë ¥ ì •ê·œí™”
            normalize_result = self.input_normalizer.process_query(user_input)
            print(f"ì •ê·œí™” ê²°ê³¼: {normalize_result}")
            
            # 3. ì¿¼ë¦¬ ìƒì„±
            query_result = self.query_maker.process_query(user_input)
            print(f"ì¿¼ë¦¬ ìƒì„± ê²°ê³¼: {query_result}")
            
            # 4. í•„í„° ìƒì„±
            filter_result = self.filter_generator.process_query(user_input)
            print(f"í•„í„° ìƒì„± ê²°ê³¼: {filter_result}")
            
            self.assertTrue(True)  # ëª¨ë“  ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë¨
        else:
            print("ì…ë ¥ì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ")

    def test_complete_rag_pipeline(self):
        """ì™„ì „í•œ RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì €ì¥
        create_doc_upsert(self.client, self.test_collection, self.sample_doc)
        
        user_query = "KAISTì—ì„œ RAG ì—°êµ¬ë¥¼ í•˜ëŠ” ì‚¬ëŒì´ ëˆ„êµ¬ì¸ê°€ìš”?"
        
        print(f"ì‚¬ìš©ì ì§ˆë¬¸: {user_query}")
        
        # 1. ì…ë ¥ ê²€ì¦
        check_result = self.input_checker.process_query(user_query)
        print(f"1. ì…ë ¥ ê²€ì¦: {check_result.get('is_valid', False)}")
        
        if not check_result.get('is_valid', False):
            print("ì…ë ¥ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        
        # 2. ì…ë ¥ ì •ê·œí™”
        normalize_result = self.input_normalizer.process_query(user_query)
        normalized_query = normalize_result.get('normalized_query', user_query)
        print(f"2. ì •ê·œí™”ëœ ì¿¼ë¦¬: {normalized_query}")
        
        # 3. í•„í„° ìƒì„±
        filter_result = self.filter_generator.process_query(user_query)
        print(f"3. ìƒì„±ëœ í•„í„°: {filter_result}")
        
        # 4. ë²¡í„° ê²€ìƒ‰
        search_results = search_doc(self.client, normalized_query, self.test_collection, k=3)
        print(f"4. ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(search_results)}")
        
        # 5. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
        context_docs = []
        for hit in search_results:
            context_docs.append({
                'text': hit.payload.get('text', ''),
                'title': hit.payload.get('title', ''),
                'author': hit.payload.get('author', ''),
                'score': hit.score
            })
        
        print("5. ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸:")
        for i, doc in enumerate(context_docs):
            print(f"   {i+1}. {doc['title']} (ì ìˆ˜: {doc['score']:.4f})")
            print(f"      ì €ì: {doc['author']}")
            print(f"      ë‚´ìš©: {doc['text'][:100]}...")
        
        # 6. ìµœì¢… ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        context_text = "\n".join([f"ë¬¸ì„œ {i+1}: {doc['text']}" for i, doc in enumerate(context_docs)])
        
        final_prompt = f"""ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤:

{context_text}

ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

ìœ„ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

        print(f"6. ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(final_prompt)} ë¬¸ì")
        
        # ì‘ë‹µ ìƒì„± (ì‹¤ì œë¡œëŠ” LLMì„ í˜¸ì¶œí•˜ì—¬ ìƒì„±)
        # ì—¬ê¸°ì„œëŠ” í…ŒìŠ¤íŠ¸ì´ë¯€ë¡œ ìƒ˜í”Œ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´
        sample_response = f"ê²€ìƒ‰ ê²°ê³¼ì— ë”°ë¥´ë©´, KAIST ì „ì‚°í•™ë¶€ì—ì„œ RAG ì—°êµ¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì—°êµ¬ìëŠ” {self.sample_doc['author']}ì…ë‹ˆë‹¤. í•´ë‹¹ ì—°êµ¬ëŠ” {self.sample_doc['date']}ì— ë°œí‘œë˜ì—ˆìœ¼ë©°, RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ í–¥ìƒì— ê´€í•œ ë‚´ìš©ì„ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤."
        
        print(f"7. ìµœì¢… ì‘ë‹µ: {sample_response}")
        
        # í…ŒìŠ¤íŠ¸ ê²€ì¦
        self.assertGreater(len(search_results), 0)
        self.assertGreater(len(context_text), 0)
        self.assertGreater(len(sample_response), 0)

def run_rag_demo():
    """RAG ì‹œìŠ¤í…œ ë°ëª¨ ì‹¤í–‰"""
    load_dotenv()
    
    # í´ë¼ì´ì–¸íŠ¸ ë° ì±—ë´‡ ì´ˆê¸°í™”
    client = QdrantClient(
        url="https://7eb854c4-8645-4c1f-ae73-609313fb8842.us-east4-0.gcp.cloud.qdrant.io",
        api_key=os.environ.get('QDRANT_API_KEY')
    )
    
    api_key = os.environ.get('GOOGLE_API_KEY')
    input_checker = InputChecker(api_key)
    input_normalizer = InputNormalizer(api_key)
    filter_generator = FilterGenerator(api_key)
    
    collection_name = "portal.job"
    
    print("=== KAIST ì „ì‚°í•™ë¶€ RAG ì‹œìŠ¤í…œ ë°ëª¨ ===")
    
    while True:
        user_query = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit'): ")
        if user_query.lower() == 'quit':
            break
        
        print(f"\nğŸ“ ì‚¬ìš©ì ì§ˆë¬¸: {user_query}")
        
        # 1. ì…ë ¥ ê²€ì¦
        print("ğŸ” ì…ë ¥ ê²€ì¦ ì¤‘...")
        check_result = input_checker.process_query(user_query)
        
        if not check_result.get('is_valid', False):
            print(f"âŒ ì…ë ¥ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {check_result.get('reason', '')}")
            continue
        
        # 2. ì…ë ¥ ì •ê·œí™”
        print("âœï¸  ì…ë ¥ ì •ê·œí™” ì¤‘...")
        normalize_result = input_normalizer.process_query(user_query)
        normalized_query = normalize_result.get('normalized_query', user_query)
        
        # 3. ë²¡í„° ê²€ìƒ‰
        print("ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
        search_results = search_doc(client, normalized_query, collection_name, k=3)
        
        if not search_results:
            print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        print(f"âœ… {len(search_results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:")
        
        for i, hit in enumerate(search_results[:3]):
            print(f"\nğŸ“„ ë¬¸ì„œ {i+1} (ìœ ì‚¬ë„: {hit.score:.4f})")
            print(f"   ì œëª©: {hit.payload.get('title', 'N/A')}")
            print(f"   ì €ì: {hit.payload.get('author', 'N/A')}")
            print(f"   ë‚ ì§œ: {hit.payload.get('date', 'N/A')}")
            print(f"   ë‚´ìš©: {hit.payload.get('text', '')[:150]}...")
        
        print(f"\nğŸ’¡ ì •ê·œí™”ëœ ê²€ìƒ‰ì–´: {normalized_query}")

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    # unittest.main()
    
    # ë°ëª¨ ì‹¤í–‰
    run_rag_demo()
