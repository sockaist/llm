"""
ë²¤ì¹˜ë§ˆí¬ í”„ë ˆì„ì›Œí¬
- ì‘ë‹µ ì‹œê°„, ì²˜ë¦¬ëŸ‰, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ê²€ìƒ‰ í’ˆì§ˆ(Recall/Precision) ì¸¡ì •
- ê° ìµœì í™” ì „í›„ ë¹„êµ ë°ì´í„° ìˆ˜ì§‘
"""

import time
import psutil
import asyncio
from typing import List, Dict
from dataclasses import dataclass
import json
from datetime import datetime
import numpy as np
import os


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”"""

    timestamp: str
    phase: str  # "baseline", "quantization", "hybrid_search" ë“±
    avg_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    qps: float  # Queries per second
    memory_mb: float
    recall_at_10: float
    precision_at_10: float
    ndcg_at_10: float  # Normalized Discounted Cumulative Gain
    avg_confidence: float  # Average confidence score


class PerformanceBenchmark:
    def __init__(self, test_queries_path: str = "src/tests/test_queries.json"):
        """
        test_queries.json í˜•ì‹:
        [
            {
                "query": "ë¨¸ì‹ ëŸ¬ë‹ì´ë€?",
                "expected_doc_ids": ["doc_123", "doc_456"],  # ê´€ë ¨ ë¬¸ì„œ ID
                "query_type": "semantic"  # semantic, keyword, hybrid
            }
        ]
        """
        self.test_queries = self._load_test_queries(test_queries_path)
        self.results_history = []

    def _load_test_queries(self, path: str) -> List[Dict]:
        if not os.path.exists(path):
            print(f"Warning: Test queries file not found at {path}")
            return []
        with open(path, "r") as f:
            return json.load(f)

    async def run_latency_benchmark(self, vector_db, num_queries: int = None) -> Dict:
        """
        ì‘ë‹µ ì‹œê°„ ì¸¡ì •
        - ê° ì¿¼ë¦¬ë¥¼ ìˆœì°¨ ì‹¤í–‰í•˜ì—¬ í‰ê· /P95/P99 ê³„ì‚°
        - ì½œë“œ ìŠ¤íƒ€íŠ¸ ì œê±°ë¥¼ ìœ„í•´ ì›œì—… ì¿¼ë¦¬ 10ê°œ ë¨¼ì € ì‹¤í–‰
        """
        if not self.test_queries:
            return {"avg": 0, "p95": 0, "p99": 0, "min": 0, "max": 0}

        if num_queries is None:
            num_queries = len(self.test_queries)

        latencies = []

        # Warm-up: ìºì‹œ ì›Œë°
        print("Warming up...")
        for _ in range(min(10, len(self.test_queries))):
            await vector_db.search(self.test_queries[0]["query"])

        # ì‹¤ì œ ì¸¡ì •
        print(f"Running latency test ({num_queries} queries)...")
        for i in range(num_queries):
            query = self.test_queries[i % len(self.test_queries)]["query"]
            start = time.perf_counter()
            await vector_db.search(query, top_k=10)
            latency = (time.perf_counter() - start) * 1000  # ms
            latencies.append(latency)

        return {
            "avg": sum(latencies) / len(latencies),
            "p95": self._percentile(latencies, 95),
            "p99": self._percentile(latencies, 99),
            "min": min(latencies),
            "max": max(latencies),
        }

    async def run_throughput_benchmark(
        self, vector_db, duration_sec: int = 60
    ) -> float:
        """
        ì²˜ë¦¬ëŸ‰ ì¸¡ì • (QPS)
        - ì§€ì •ëœ ì‹œê°„ ë™ì•ˆ ìµœëŒ€í•œ ë§ì€ ì¿¼ë¦¬ ë³‘ë ¬ ì²˜ë¦¬
        - ì‹¤ì œ í”„ë¡œë•ì…˜ ë¶€í•˜ ì‹œë®¬ë ˆì´ì…˜
        """
        if not self.test_queries:
            return 0.0

        print(f"Running throughput test ({duration_sec}s)...")
        query_count = 0
        start_time = time.time()

        async def worker():
            nonlocal query_count
            while time.time() - start_time < duration_sec:
                query = self.test_queries[query_count % len(self.test_queries)]["query"]
                await vector_db.search(query)
                query_count += 1

        # 10ê°œ ì›Œì»¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬
        await asyncio.gather(*[worker() for _ in range(10)])

        return query_count / duration_sec

    async def run_quality_benchmark(self, vector_db) -> Dict:
        """
        ê²€ìƒ‰ í’ˆì§ˆ ì¸¡ì •
        - Recall@K: ê´€ë ¨ ë¬¸ì„œë¥¼ ì–¼ë§ˆë‚˜ ì°¾ì•˜ëŠ”ê°€
        - Precision@K: ì°¾ì€ ë¬¸ì„œ ì¤‘ ê´€ë ¨ ë¬¸ì„œ ë¹„ìœ¨
        - NDCG@K: ìˆœìœ„ê¹Œì§€ ê³ ë ¤í•œ í’ˆì§ˆ
        """
        if not self.test_queries:
            return {
                "recall_at_10": 0,
                "precision_at_10": 0,
                "ndcg_at_10": 0,
                "avg_confidence": 0.0,
            }

        print("Running quality benchmark...")
        recall_scores = []
        precision_scores = []
        ndcg_scores = []
        confidence_scores = []

        for test_case in self.test_queries:
            if not test_case.get("expected_doc_ids"):
                continue

            results = await vector_db.search(test_case["query"], top_k=10)

            # Extract IDs and Scores
            # Result objects usually have .id and .score attributes
            result_ids = []
            top_score = 0.0

            if results:
                # Check structure of first result to handle object vs dict
                first = results[0]
                if hasattr(first, "score"):
                    top_score = first.score
                elif isinstance(first, dict):
                    top_score = first.get("score", 0.0)

                confidence_scores.append(top_score)

                result_ids = [
                    r.id if hasattr(r, "id") else r.get("id") for r in results
                ]
            else:
                confidence_scores.append(0.0)

            expected_ids = set(test_case["expected_doc_ids"])

            # Recall@10
            hits = len(set(result_ids) & expected_ids)
            recall = hits / len(expected_ids) if expected_ids else 0
            recall_scores.append(recall)

            # Precision@10
            precision = hits / 10
            precision_scores.append(precision)

            # NDCG@10
            ndcg = self._calculate_ndcg(result_ids, expected_ids, k=10)
            ndcg_scores.append(ndcg)

        if not recall_scores:
            return {
                "recall_at_10": 0,
                "precision_at_10": 0,
                "ndcg_at_10": 0,
                "avg_confidence": 0.0,
            }

        return {
            "recall_at_10": sum(recall_scores) / len(recall_scores),
            "precision_at_10": sum(precision_scores) / len(precision_scores),
            "ndcg_at_10": sum(ndcg_scores) / len(ndcg_scores),
            "avg_confidence": sum(confidence_scores) / len(confidence_scores)
            if confidence_scores
            else 0.0,
        }

    async def run_memory_benchmark(self, vector_db) -> float:
        """
        ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        - ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        """
        if not self.test_queries:
            return 0.0

        print("Running memory benchmark...")
        process = psutil.Process()
        baseline = process.memory_info().rss / 1024 / 1024
        max_memory = baseline

        num_queries = min(len(self.test_queries), 50)  # Limit memory check to 50
        for i in range(num_queries):
            query = self.test_queries[i % len(self.test_queries)]["query"]
            await vector_db.search(query)
            memory = process.memory_info().rss / 1024 / 1024  # MB
            max_memory = max(max_memory, memory)

        return max_memory

    async def run_full_benchmark(self, vector_db, phase_name: str) -> BenchmarkResult:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥"""
        print(f"\n{'=' * 50}")
        print(f"Running benchmark for phase: {phase_name}")
        print(f"{'=' * 50}\n")

        latency = await self.run_latency_benchmark(vector_db)
        qps = await self.run_throughput_benchmark(vector_db)
        quality = await self.run_quality_benchmark(vector_db)
        memory = await self.run_memory_benchmark(vector_db)

        result = BenchmarkResult(
            timestamp=datetime.now().isoformat(),
            phase=phase_name,
            avg_latency_ms=latency["avg"],
            p95_latency_ms=latency["p95"],
            p99_latency_ms=latency["p99"],
            qps=qps,
            memory_mb=memory,
            recall_at_10=quality["recall_at_10"],
            precision_at_10=quality["precision_at_10"],
            ndcg_at_10=quality["ndcg_at_10"],
            avg_confidence=quality.get("avg_confidence", 0.0),
        )

        self.results_history.append(result)
        self._save_results()
        self._print_results(result)

        return result

    def _save_results(self):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        os.makedirs("src/tests", exist_ok=True)
        try:
            with open("src/tests/benchmark_results.json", "w") as f:
                # Convert dataclass to dict
                json.dump([vars(r) for r in self.results_history], f, indent=2)
        except Exception as e:
            print(f"Failed to save benchmark results: {e}")

    def _print_results(self, result: BenchmarkResult):
        """ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
        print(f"\nğŸ“Š Benchmark Results - {result.phase}")
        print(
            f"â”œâ”€ Latency: {result.avg_latency_ms:.1f}ms (P95: {result.p95_latency_ms:.1f}ms)"
        )
        print(f"â”œâ”€ Throughput: {result.qps:.1f} QPS")
        print(f"â”œâ”€ Memory: {result.memory_mb:.1f} MB")
        print(f"â”œâ”€ Recall@10: {result.recall_at_10:.3f}")
        print(f"â”œâ”€ NDCG@10: {result.ndcg_at_10:.3f}")
        print(f"â””â”€ Avg Confidence: {result.avg_confidence:.3f}\n")

    def compare_phases(self, baseline: str, optimized: str):
        """ë‘ phase ê°„ ì„±ëŠ¥ ë¹„êµ"""
        try:
            baseline_result = next(
                r for r in self.results_history if r.phase == baseline
            )
            optimized_result = next(
                r for r in self.results_history if r.phase == optimized
            )
        except StopIteration:
            print("Comparison failed: Phase not found in history.")
            return

        print(f"\nğŸ“ˆ Performance Improvement: {baseline} â†’ {optimized}")
        print(
            f"â”œâ”€ Latency: {self._improvement(baseline_result.avg_latency_ms, optimized_result.avg_latency_ms)}"
        )
        print(
            f"â”œâ”€ Throughput: {self._improvement(baseline_result.qps, optimized_result.qps, inverse=True)}"
        )
        print(
            f"â”œâ”€ Memory: {self._improvement(baseline_result.memory_mb, optimized_result.memory_mb)}"
        )
        print(
            f"â””â”€ Recall: {self._improvement(baseline_result.recall_at_10, optimized_result.recall_at_10, inverse=True)}\n"
        )

    def _improvement(self, before: float, after: float, inverse: bool = False) -> str:
        """ê°œì„ ìœ¨ ê³„ì‚° (inverse=Trueë©´ ì¦ê°€ê°€ ì¢‹ìŒ)"""
        if before == 0:
            return "N/A"
        if inverse:
            pct = ((after - before) / before) * 100
            return f"{pct:+.1f}% {'[OK]' if pct > 0 else '[FAIL]'}"
        else:
            pct = ((before - after) / before) * 100
            # for latency/memory, lower is better (negative pct is good)
            return f"{pct:+.1f}% {'[OK]' if pct < 0 else '[FAIL]'}"

    @staticmethod
    def _percentile(data: List[float], percentile: int) -> float:
        """ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°"""
        if not data:
            return 0.0
        sorted_data = sorted(data)
        idx = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(idx, len(sorted_data) - 1)]

    @staticmethod
    def _calculate_ndcg(result_ids: List[str], expected_ids: set, k: int) -> float:
        """NDCG ê³„ì‚° - ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ê²€ìƒ‰ í’ˆì§ˆ"""
        dcg = sum(
            [
                1 / np.log2(i + 2)
                for i, doc_id in enumerate(result_ids[:k])
                if doc_id in expected_ids
            ]
        )
        idcg = sum([1 / np.log2(i + 2) for i in range(min(k, len(expected_ids)))])
        return dcg / idcg if idcg > 0 else 0
