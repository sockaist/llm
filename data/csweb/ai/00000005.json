{
    "title": "초거대 언어모델 추론 서비스 제공을 위한 HW／SW 공동 시뮬레이션 인프라(LLMServingSim： A HW／SW Co-Simulation Infrastructure for LLM Inference Serving at Scale)",
    "date": "2024-10-11T00:00:00Z",
    "link": "https://cs.kaist.ac.kr/board/view?bbs_id=research&bbs_sn=11230&page=1&skey=subject&svalue=&menu=265",
    "content": "전산학부 박종세 교수 연구팀, 컴퓨터 시스템 워크로드 특성화 분야에서 세계적으로 권위 있는 학술대회인 IISWC 2024에‘최우수 논문상’,‘최우수 연구 기록물상 ’ 동시 수상\nLLM 추론 서비스를 위한 GPU, NPU, PIM 등 다양한 AI반도체(HW)와 이를 위한 시스템 소프트웨어(SW)를 동시에 시뮬레이션할 수 있는 오픈소스 도구(Tool) 개발\n향후 생성형 AI를 위한 클라우드 시스템 연구에 기여\n그림 1. 개발한 시뮬레이션 인프라의 시뮬레이션 흐름. 스케줄러부터 시작하여 ASTRA-Sim까지가 한 회차의 흐름이며 이 과정이 계속 반복된다. 매 회차마다 스케줄러에서 기존 작업의 변화와 새로운 작업을 반영하여 현재 회차 시뮬레이션 계획을 세움으로써 초거대 언어모델의 동적 특성을 지원한다.\n□ 연구개요\n1. 연구 배경\n초거대 언어모델(LLM) 추론 서비스는 ChatGPT, Copilot, Gemini, Claude 등 여러 기업들이 제공하고 있으며, 단순한 챗봇을 넘어 이미지, 음성, 동영상까지 생성할 수 있는 생성형 AI(Generative AI)로 발전하고 있다. 하지만 이러한 초거대 언어모델 추론 서비스를 실행하려면 수천 개의 GPU가 탑재된 대규모 서버 시스템이 필요하다. 이처럼, 초거대 언어모델 추론에는 막대한 자원이 요구되기 때문에, 하드웨어와 소프트웨어 기술이 빠르게 발전하고 있다. 그래서 이러한 하드웨어와 소프트웨어를 대규모로 실험하기 위한 시뮬레이션 인프라가 필수적이다. 그러나 기존의 AI 시스템 시뮬레이터는 여러 한계점을 가지고 있다. 첫 번째는 이종 가속기의 미지원이다. 가속기는 특정 작업에 특화되어 있기 때문에 GPU에 비해 좋은 성능과 에너지 효율을 보인다. 그래서 현재 GPU뿐만 아니라 NPU(신경망처리장치)와 PIM(지능형메모리반도체)과 같이 AI를 위한 많은 가속기에 대한 연구가 진행되고 있으며 이를 적극적으로 활용해야 한다. 두 번째는 초거대 언어모델의 동적 특성 미지원이다. 초거대 언어모델은 자기회귀적으로 결과를 생성하기 때문에 매 회차 마다 입력의 크기가 변하고 사용자들이 임의의 시간에 추론 요청을 하기 때문에 작업이 시간에 따라 변한다. 본 연구에서 이러한 문제점들을 해결하기 위해 여러 이종 하드웨어를 지원하고 초거대 언어모델 추론 관련 소프트웨어 기술들을 사용할 수 있는 초거대 언어모델 시뮬레이션 인프라를 개발하고자 하였다.\n그림 2. 개발한 시뮬레이션 인프라의 실행 시간 단축 기술. 초거대 언어모델의 반복 구조를 활용하여 6개의 레이어만 실행한 후 그 결과물을 합쳐 전체 모델에 대한 결과물을 만들고, 재사용을 통해 실행 시간을 획기적으로 단축시켰다.\n2. 연구 내용\n초거대 언어모델의 경우 매 회차마다 작업이 변하는 동적 특성을 가지고 있다. 이를 위해 본 연구는 여러 시뮬레이션 구성 요소를 반복 순환하는 구조로 설계하였다. 매 회차마다 스케줄러가 현재 가지고 있는 작업과 새로운 사용자의 작업을 분석하고 현재 회차에 그것을 반영한다. 또한 초거대 언어모델의 구조적 특성상 특정 레이어에서 많은 사용자의 요청을 한 번에 처리하는 배칭(Batching)이 불가능하다. 이를 위해서 본 연구는 특정 레이어를 인식하여 특정 레이어에서는 작업을 요청 단위로 분할하고 나머지 레이어에서는 배칭을 하는 방법으로 해결했다. 기존 시뮬레이터의 또 다른 문제는 실행 시간이 매우 오래 걸린다는 점이다. 본 연구에서는 반복되는 초거대 언어모델의 구조적 특성을 활용하여 중복되는 작업을 줄이고, 초거대 언어모델 추론의 지역성(Locality)를 활용하여 작업 결과물을 재사용하는 방식으로 실행 시간을 획기적으로 단축했다. 마지막으로 본 연구에서는 이종 가속기의 지원을 위해 연산자를 각 가속기에 할당하는 모듈과 할당된 연산자를 스케쥴링하는 모듈을 만들어 다양한 이종 가속기를 지원할 수 있도록 하였다. 본 연구는 GPU를 사용한 실제 서버 시스템과 비교하여 높은 유사도를 통해 정확성을 검증하였고 여러 다른 기존 시뮬레이터와의 실행 시간 비교에서 압도적인 시간 단축을 보여주었다.\n그림 3. 개발한 시뮬레이션 인프라의 이종 가속기 지원. 그림의 경우 NPU와 PIM으로 이루어진 이종 시스템의 예시로, 초거대 언어모델의 Attention 레이어 연산자를 PIM에 할당 후 레이어간 종속성에 따라 스케줄링을 진행한다.\n3. 기대 효과\n현재 전 세계적으로 많은 기업들이 초거대 언어 모델을 활용한 AI 서비스를 개발하고 있으며, 국내에서도 SK텔레콤의 에이닷, 네이버의 클로바, 스캐터랩의 이루다 등 다양한 AI 서비스가 활성화되고 있다. 더불어 AI 반도체 연구도 활발히 진행되고 있으며, 특히 리벨리온, 퓨리오사AI, 하이퍼엑셀과 같은 국내 AI 반도체 스타트업뿐만 아니라 삼성전자와 SK하이닉스에서도 PIM 기술 개발이 이루어지고 있다. 이러한 AI 반도체의 발전과 더불어, 본 연구에서 제안한 시뮬레이션 인프라의 다양한 가속기와 시스템 구성을 실험할 수 있는 능력은 미래의 대규모 AI 서비스 시스템 설계와 최적화에 중요한 역할을 할 것으로 기대된다. 이를 통해 더 나은 성능과 효율성을 갖춘 AI 시스템의 개발이 가능해지며, 학문적 및 산업적 발전에 크게 기여할 수 있을 것이다.",
    "tag": "csweb.ai",
    "id": 5
}