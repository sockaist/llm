{
    "title": "거대언어모델(LLM) 서빙 가속을 위한 이기종 AI 반도체 기반 클라우드 시스템 연구",
    "date": "2025-03-31T00:00:00Z",
    "link": "https://cs.kaist.ac.kr/board/view?bbs_id=research&bbs_sn=11412&page=1&skey=subject&svalue=&menu=265",
    "content": "책임교수\n: 박종세 교수\n참여연구자\n: 허구슬, 이상엽, 조재홍, 최현민, 이상현, 김민수, 함형규(POSTECH), 김광선(POSTECH), Divya Mahajan (Georgia Institute of Technology)\n연구 개요\n본 연구는 거대언어모델(LLM)의 서빙을 효율화하기 위해 GPU, NPU, PIM 등 이기종 AI 반도체의 특성을 고려한 하드웨어-소프트웨어 공동 설계를 제안하였다. 특히 연산 집약적인 GEMM과 메모리 대역폭에 의존적인 GEMV 연산의 특성 차이를 반영해, 두 연산을 병렬 처리하는 아키텍처를 설계하고, 알고리즘적 기법을 통해 자원 활용도를 극대화하였다. 이러한 접근은 LLM 추론 과정에서 발생하는 병목 현상을 해결하며, 대규모 AI 모델 서빙의 성능과 확장성을 크게 향상시켰다.\n또한, 반복 연산 재활용 및 동적 배치 스케줄링 기능을 포함한 시뮬레이션 인프라를 구축하여 다양한 시스템 설계 및 성능 평가를 빠르고 정밀하게 수행할 수 있도록 하였다.\n주요 연구 내용\n그림1. NeuPIMs 시스템의 개요\nNeuPIMs 아키텍처 설계\n:\nGEMM 연산에 최적화된 NPU와 GEMV 연산에 적합한 PIM을 결합하여, 연산 자원을 효율적으로 분산 처리할 수 있는 병렬 구조를 구현함.\nLLMServingSim 시뮬레이션 인프라 개발\n:\n반복 연산 재활용 및 동적 배치 스케줄링을 기반으로 시뮬레이션 속도를 향상시키고, 다양한 설계안에 대한 성능 분석을 지원함.\n기대 효과\n그림2. LLMServingSim의 작업 흐름도\nLLM 추론 성능 극대화\n:\nNeuPIMs 구조는 GEMM/GEMV 병렬 처리를 통해 기존 GPU 기반 시스템 대비 최대 3배 이상의 처리량 향상을 달성함.\nAI 클라우드 서빙 기반 강화\n:\n본 연구는 대규모 LLM 기반 AI 서비스의 실질적인 서빙 효율 향상에 기여하며, 차세대 AI 클라우드 인프라 설계의 핵심 기술로 활용될 수 있음.",
    "tag": "csweb.ai",
    "id": 3
}