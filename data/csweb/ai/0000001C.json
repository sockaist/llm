{
    "title": "KAIST, 손객체 상호작용시 삼차원 손자세 인식 기술 개발",
    "date": "2022-06-16T00:00:00Z",
    "link": "https://cs.kaist.ac.kr/board/view?bbs_id=research&bbs_sn=10314&page=1&skey=subject&svalue=&menu=265",
    "content": "한국과학기술원(KAIST) 연구진이 주도하고, 영국 임페리얼 칼리지 (Imperial College London), UNIST가 공동 연구에 참여하여, 손과 물체가 상호 가림이 있는 상황에서 삼차원 손자세를 인식하는 기술을 개발했다.\n이전까지 손과 물체가 따로 있는 상황에서 각각의 자세를 인식하는 연구가 주를 이루었으며, 손 물체 상호작용시 가림 현상이 크게 나타나 기존 방법은 자세 인식율이 현저하게 떨어진다. 이번 개발된 기술은 손객체 상호작용이 빈번이 일어나는 환경에서 획기적인 손동작 기반 AR/VR 인터페이스를 제공할것으로 전망된다.\n김태균 KAIST 전산학부 교수팀(컴퓨터 비전 및 러닝 랩)(ICL 겸직), ICL의 백승렬 연구원, UNIST의 김광인 교수가 공동으로 기술을 개발했다.\n<왼쪽부터, 입력 영상, 메쉬 모델 정합 결과, GAN생성 영상, 메쉬 모델과 GAN을 결합하여 객체를 제거한 손영상, 손자세 인식 결과>\n기존 손객체 상호작용 데이터셋은 사이즈가 작고, 실제 영상은 가림현상으로 삼차원 손자세 레이블의 정확도가 떨어지는 문제점이 있다. 이에 반해 손 영상 데이터셋은 충분히 크고, 다양한 손자세가 포함되어 있다. 이러한 부분에 착안하여 제안한 핵심 아이디어는 손객체 상호작용에서 객체를 제거한 손만의 영상을 얻어내는 새로운 domain 변환 기술이다.\n삼차원 손모델을 입력 영상에 정합하는 기술과, 적대적 생성 신경망 (Generative Adversarial Network)을 함께 사용하여, 입력 영상과 최적의 정합을 보이면서도, 제거한 물체 영역의 픽셀 정보를 복원해 내는 기술을 개발하였다. 제안한 domain 변환 네트웍은 삼차원 손자세 인식 모델과 함께 end-to-end 학습이 가능하다.\n다양한 벤치마크 실험에서 기존의 SOTA 모델을 탁월하게 뛰어넘는 성능을 얻었으며, 제안한 모델은 주로 손만의 영상을 사용하여 지도학습하고, 손객체 영상은 비지도 학습이 가능해, 더많은 영상으로 학습이 가능해 지는 이점이 있다. 김태균 교수는 “지금까지 수많은 메이저 산학 기관에서, 삼차원 손자세 인식에 대한 연구 개발을 진행하였으나, 가림 현상이 있을시 그 성능이 크게 저하되는 문제가 있었다”라며 “본 연구에서 제안한 domain 변환 기술을 사용하면 손객체 상호작용시 얻기 어려운 학습 데이터 문제를 획기적으로 해결하기 때문에”, “향후 Metaverse, AR/VR 업계에 큰 영향을 미칠것으로 예상한다”고 말했다.\n관련 연구는 컴퓨터비전 최고 학회인 CVPR 2020에서 구두 발표되었고 (5.7％ accept rate), best paper award finalist로 선정되었다 (6,656편의 제출 논문중 26편만이 채택).\n<위쪽부터, 입력 영상, 본방법을 적용 객체를 제거한 손영상, 손자세 인식 결과>",
    "tag": "csweb.ai",
    "id": 28
}