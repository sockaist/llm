{
    "title": "[ERC AI Seminar＃12] 4／19(Tue) 4pm, Kyunghyun Cho(New York University／Associate Professor),“Learned data augmentation in natural language processing”",
    "date": "2022-04-19T00:00:00Z",
    "link": "https://cs.kaist.ac.kr/board/view?bbs_id=events&bbs_sn=10193&page=1&skey=&svalue=&menu=86",
    "content": "title:[ERC AI Seminar＃12] 4／19(Tue) 4pm, Kyunghyun Cho(New York University／Associate Professor),“Learned data augmentation in natural language processing” date:2022-04-19T00:00:00Z link:https://cs.kaist.ac.kr/board/view?bbs_id=events&bbs_sn=10193&page=1&skey=&svalue=&menu=86 content:1. Date & Time : 16:00, Apr 19, 2022 2. Place : - Offline : Information & Electronics Building (E3-1) Room 4443 OSS Multimedia - Online : https://kaist.zoom.us/my/aliceatkaist 3. Speaker : Kyunghyun Cho (New York University / Associate Professor, Genentech / Senior Director of Frontier Research) 4. Title : Learned data augmentation in natural language processing 5. Abstract : Data augmentation has been found as a key aspect in modern machine learning. Especially in domains and problems in which we have knowledge of important invariances and equivariances, we can design a data augmentation procedure to encourage a machine learning model to encode those invariances and equivairances. In the case of natural language processing, it is unfortunately difficult to come up with such a data augmentation procedure, as our knowledge of invariances and equivariances is limited. Instead, we need to rely on a vast amount of unlabelled data to \"learn to augment\" data. In this talk, I will talk about two different approaches. In the first approach, we use a standard masked language model to produce a set of samples given a training sequence to augment the data along the data (text) manifold learned by the masked language model. In the second approach, we design an algorithm that learns to interpolate two text snippets, allowing us to use a successful data augmentation method, called mixup, which requires a mechanism to mix in contents from two different examples. If time permits, I will talk briefly about how this learned data augmentation can be used to predict generalization as well. 6. Bio : Kyunghyun Cho is an associate professor of computer science and data science at New York University and CIFAR Fellow of Learning in Machines & Brains. He is also a senior director of frontier research at the Prescient Design team within Genentech Research & Early Development (gRED). He was a research scientist at Facebook AI Research from June 2017 to May 2020 and a postdoctoral fellow at University of Montreal until Summer 2015 under the supervision of Prof. Yoshua Bengio, after receiving PhD and MSc degrees from Aalto University April 2011 and April 2014, respectively, under the supervision of Prof. Juha Karhunen, Dr. Tapani Raiko and Dr. Alexander Ilin. He tries his best to find a balance among machine learning, natural language processing, and life, but almost always fails to do so. 7. Language : English location: https://kaist.zoom.us/my/aliceatkaist tag:csweb.calendar",
    "location": " https://kaist.zoom.us/my/aliceatkaist",
    "tag": "csweb.calendar",
    "id": 164
}