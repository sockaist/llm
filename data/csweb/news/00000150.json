{
    "title": "학부생 허미나(지도교수 김주호) URP 최우수상 수상",
    "date": "2020-09-11T00:00:00Z",
    "link": "https://cs.kaist.ac.kr/board/view?bbs_id=news&bbs_sn=9352&page=9&skey=subject&svalue=&menu=83",
    "content": "KAIST 전산학부 학사과정 허미나 학생이 8월 27일(목), 온라인으로 개최된 2020 겨울/봄학기 URP(학부연구프로그램) 프로그램 워크샵 제 3 발표장(전자 및 전기공학부, 전산학부) 최우수상을 수상했다. (지도교수: 김주호, 조교: 장민석 박사과정).\nURP 프로그램은 교육부와 한국과학창의재단에서 학부생들의 창의적 연구역량을 향상시키고, 자기 주도적으로 연구할 수 있는 기회를 제공하기 위한 학부생 연구프로그램 지원 사업이다.\nTitle: Data Modeling Techniques for Supporting Conversational Interaction in Video Tutorials\n제목: 튜토리얼 비디오 기반 대화형 인터랙션을 위한 데이터 모델 기법\nAbstract:\nWhen watching video tutorials, viewers often need to pause, rewind, or skip to clarify misunderstandings, and to find missing or anticipated information. The target scene can be referenced in terms of their content, or by their temporal location. Voice interfaces are theoretically useful for controlling video tutorials, especially when the hands are tied by following physical tasks. However, they only support remote-control like commands such as “skip 30 seconds” that are based on temporal references. As a result, viewers must remember a sequence of events, their temporal locations and maintain a mapping between them while following the tutorial. We present RubySlippers, a multimodal system that allows users to navigate video tutorials by conversational description of the content, while keeping the time-driven navigation. The system runs on an NLP pipeline which compares the semantic composition of the user’s utterance and that of the video content to populate navigation target candidates. Our evaluation with 12 participants shows that users are able to navigate quicker with lower mental demand with RubySlippers than existing time-driven voice interfaces.\n수상을 축하드립니다.",
    "tag": "csweb.news",
    "id": 336
}